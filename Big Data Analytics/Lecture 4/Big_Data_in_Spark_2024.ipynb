{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_main_arcada.png\" style=\"width:1400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lecture 3: Big Data Engineering\n",
    "\n",
    "\n",
    "## Instructor:\n",
    "Anton Akusok <br/> \n",
    "email:  anton.akusok@arcada.fi<br/>\n",
    "message: \"Anton Akusok\" @ Microsoft Treams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Content \n",
    "\n",
    "1. Big Data Analytics technologies\n",
    "\n",
    "2. Introduction to Spark (https://andfanilo.github.io/pyspark-interactive-lecture/#/)\n",
    "\n",
    "3. Exercise: Install Spark and look at the data\n",
    "\n",
    "3. Basics of PySpark\n",
    "   (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)\n",
    "\n",
    "5. PySpark and SQL\n",
    "   (https://docs.databricks.com/en/getting-started/dataframes-python.html)\n",
    "\n",
    "6. PySpark operations demo\n",
    "\n",
    "7. Exercise/homework: Analyze shopping trends data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 1. Tech around Big Data Analytics\n",
    "\n",
    "![spark book](images/book.png)\n",
    "\n",
    "https://www.amazon.de/-/en/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Basic idea\n",
    "\n",
    "- store large data is almost always in \"bucket\" / Cloud object storage, like Amazon S3\n",
    "- interface to look at the data while processing it / interactive data visualization ",
    "\n",
    "two options: Jupyter / Databricks notebook, or an SQL query web thing\n",
    "- schedule data processing pipeline: ",
    "\n",
    "either run a script e.g. AirFlow runs Jupyter notebooks, ",
    "\n",
    "or define transformations and they run as needed e.g. table graph in \"dbt\"\n",
    "- ability to write code and integrate with other services, ",
    "\n",
    "\"easy visualization tools\" will always have a few missing things that are deal breakers for industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Options\n",
    "- open source tools like SQL, Spark, dbt\n",
    "- paid special tools like Tableau\n",
    "- cloud providers offering tools in a nice wrapping: AWS SageMaker, RedShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Surrounding tools\n",
    "- NoSQL databases - data storage and queries, can use Spark for processing\n",
    "- Graph databases\n",
    "- Stream data processing (Kafka, Flink). Spark Streaming is in-between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data in-memory: Dataframe\n",
    "- There are Pandas dataframe and Spark dataframe, they are different but can convert into each other.\n",
    "- Attention: Pandas is one machine in-memory data and Spark is distributed across many machines. Easy to get out of memory converting Spark to Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Data storage\n",
    "- Distributed data storage (always with large data). Has many files. Each one file reads at average speed, but all files at once can be read amazingly fast.\n",
    "\n",
    "  \n",
    "- Columnar storage format! Can load a few columns without reading everything. Examples: Parquet, Databricks Delta\n",
    "- JSON: most flexible, often the format data comes in. Always compressed (.gzip) and takes very long time to read, easily 80-90% or runtime just for reading data.\n",
    "\n",
    "- JSON Benefits: natural choice when data comes one row at a time - those rows are compressed and appended to a JSON. Always comes in many smaller files that are read very fast (limited by CPU)\n",
    "- JSON Drawbacks: cannot quickly read a subset of data, no defined schema, schema will change over time causing problems reading old data. Can provide your own schema to selectively read only necessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 2. Introduction to Spark \n",
    "\n",
    "https://andfanilo.github.io/pyspark-interactive-lecture/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 3. Let's run some Spark!\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Installing Spark\n",
    "\n",
    "`pip install pyspark`   \n",
    "or  \n",
    "`pip install \"pyspark[sql]\"`\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## (Py)Spark needs Java\n",
    "\n",
    "There are many Javas like OpenJDK\n",
    "\n",
    "Macos: `brew install openjdk`\n",
    "\n",
    "https://howtodoinjava.com/java/install-openjdk-on-linux-macos-windows/ \n",
    "\n",
    "May need to setup `JAVA_HOME` environmental variable - ask AI how to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# SparkContext\n",
    "\n",
    "* Main entry point of a spark application\n",
    "* SparkContext sets up everything and establishes a connection for writing Spark code\n",
    "* Used to create RDDs, access Spark services and run Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## AI assistants!\n",
    "\n",
    "Don't ask me - ask them. They are always there for you.\n",
    "\n",
    "This is the new Google / Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## AI Shadow Code\n",
    "\n",
    "Shadow code give you coding suggestions.\n",
    "\n",
    "Hint: Type a comment for what you want to do - then it gives you a better suggestion!\n",
    "\n",
    "```\n",
    "# print a greeting\n",
    "<print(\"Hello, world\")>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print a greeting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## VS Code tips and tricks\n",
    "\n",
    "1. Run a local AI model with Jan.ai and Continue.dev plugin\n",
    "\n",
    "2. Run Jupyter notebooks by installing Jupyter plugin and writing a magic cell separator string `#%%`\n",
    "\n",
    "3. Use Python brackets for multi-line code\n",
    "```\n",
    "spark.filter().select(a,b,c).join().filter().display()\n",
    "\n",
    "# is the same as:\n",
    "(\n",
    "    spark\n",
    "    .filter()\n",
    "    .select(\n",
    "        a,\n",
    "        b,\n",
    "        c\n",
    "    )\n",
    "    .join()\n",
    "    .filter()\n",
    "    .display()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Continue.dev\n",
    "\n",
    "Open config and give settings of your server. \n",
    "Simple to use servers are Jan.ai or Ollama.\n",
    "\n",
    "```\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"title\": \"Phind CodeLlama\",\n",
    "      \"provider\": \"openai-aiohttp\",\n",
    "      \"model\": \"phind-codellama-34b\",\n",
    "      \"api_base\": \"http://127.0.0.1:5000\"\n",
    "    }\n",
    "  ],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Have an AI assistant ready\n",
    "2. (optional but suggested) Connect AI Shadow Code assistant\n",
    "3. Load `data/titanic.csv` (or any other) dataset in Spark\n",
    "4. Print how many lines you have\n",
    "5. Print a few lines of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_break.png\" style=\"width:1100px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 4. Basics of PySpark\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 5. PySpark and SQL\n",
    "\n",
    "https://docs.databricks.com/en/getting-started/dataframes-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 6. PySpark operations demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Operations\n",
    "- load, print, save\n",
    "- read with schema\n",
    "- count and take a few rows\n",
    "- filter\n",
    "- select with its features: change types, rename\n",
    "- intro to functions inside \"select\"\n",
    "- groupby-agg thingy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## SQL \n",
    "- inline sql queries\n",
    "- register dataframe as table\n",
    "- general SQL\n",
    "- getting data back as dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# (more complex)\n",
    "- joins of different kinds\n",
    "- window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## UDFs and Pandas UDFs\n",
    "- writing custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 7. Exercise/homework: Analyze shopping trends data\n",
    "\n",
    "1. Load the dataset into a PySpark DataFrame, converting the purchase_amount column to a numeric type.\n",
    "2. Group the data by age and customer ID to compute total purchases for each customer in every age group.\n",
    "3. Compute statistics such as mean, median, standard deviation, and quartiles for the total purchase amounts across customers.\n",
    "4. Use PySpark's SQL API to create a view with customer IDs, total purchases.\n",
    "5. Write SQL queries to find the following:\n",
    "    1. The top 10 customers with the highest total purchases.\n",
    "    2. The average purchase amount per age for all customers, grouped by gender.\n",
    "    3. The total number of purchases made in each age across all customers.\n",
    "6. Visualize the results using Matplotlib or other libraries to create bar and line charts.\n",
    "7. Submit a Jupyter notebook with results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"images/00_thats_all.jpg\",width=1200></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "latex_metadata": {
   "author": "Leonardo Espinosa",
   "title": "Big Data Anlytics and Optimization"
  },
  "livereveal": {
   "overlay": "<div class='myheader'><h2 class='headertekst'>Big Data Analytics and Optimization (BDA Specialization) <img src='./images/00_arcada.png' width='200' height='50'></img></h2><h3 ><a href='#/9/1'>(index)</a></h3></div>",
   "progress": true,
   "scroll": true,
   "theme": "serif",
   "transition": "simple"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(0, 0, 0)",
     "rgb(255, 255, 255)"
    ]
   },
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
