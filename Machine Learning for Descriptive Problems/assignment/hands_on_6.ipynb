{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/phamt11/anaconda3/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets tiktoken wandb tqdm==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/phamt11/anaconda3/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (2.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: filelock in /home/phamt11/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/phamt11/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/phamt11/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phamt11/anaconda3/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for eli5_category contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eli5_category\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': '5lchat',\n",
       " 'title': \"Why there was a 'leap second' added to the end of 2016?\",\n",
       " 'selftext': '',\n",
       " 'category': 'Other',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['dbuoyxl', 'dbur7gi', 'dbuotht'],\n",
       "  'text': ['the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.',\n",
       "   \"The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0\",\n",
       "   \"Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.]( URL_0 ) In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.\"],\n",
       "  'score': [44, 5, 4],\n",
       "  'text_urls': [[],\n",
       "   ['http://adminhacks.com/leap-second-bugs.html'],\n",
       "   ['https://en.wikipedia.org/wiki/Newcomb%27s_Tables_of_the_Sun']]},\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#eli5 = load_dataset(\"eli5_category\", split=\"train\")\n",
    "eli5 = load_dataset(\"eli5_category\")\n",
    "\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758844cf9ad542fd97b844dba2df1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abe60377b78439d85866bf89d5afaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcdbe2545c74d89bd24edaf0bae0fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ab775952c0446ebe500cac12af2431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(transformers.__version__)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '5lchat',\n",
       " 'title': \"Why there was a 'leap second' added to the end of 2016?\",\n",
       " 'selftext': '',\n",
       " 'category': 'Other',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['dbuoyxl', 'dbur7gi', 'dbuotht'],\n",
       " 'answers.text': ['the rotation of the earth is not a constant. in fact the rotation of the earth is slowing down, which means that a full day is getting slightly longer. without leap seconds our clocks would slowly drift ever so slightly out of sync with the actual day. we could deal with this by redefining how how long 1 second is, making it slightly longer so that one day is still exactly 24*60*60 seconds. but in practice that is really inconvenient for a lot of our technology which relies on very precise timing. its easier to just move us ahead one second every couple of years or so.',\n",
       "  \"The Earth's rotation is not regular. It varies a bit, so sometimes we add a second. We do this to ensure that noon is always going to be sometime around mid-day. If we did not add leap seconds, over a very long period of time where the Earth's rotation slowly changed, noon could end up being at dusk. We want to keep 7am in the morning, noon at mid-day, 7pm around evening, etc. Though we have never had one, it's also possible to have a negative leap second. That is, taking away a second from the year. This has never happened, but if the Earth's rotation were to speed up, it could happen. The biggest thing to know about leap seconds is that they can cause computer problems. You might remember the Y2K bug. A leap second can cause similar problems, and they actually have caused problems in the past. The reason for this is that generally we expect a day to have 24 hours, and for time to always move forward. With a leap second this is not true. When writing software, programers try to think of all the possible exceptions that could happen withing their code. For example, the program might expect a word, but instead get a number. A good programmer will check for these exceptions and deal with them. However, a programer can easily forget about leap seconds and not have a fail-safe in their code for when a day have more than 24 hours. When such an exception happens, the program can produce errors or crash. It is an interesting topic, you can read more about it here: URL_0\",\n",
       "  \"Because the Earth's rotation is slowing. If you multiply 24 hours by 60 minutes by 60 seconds, you find that there are 86400 seconds per day. The problem is that our definition of the second is based on [an average that is a century old.]( URL_0 ) In modern times, the average day is about 2 thousandths of a second longer—again, because of Earth's slowing rotation. Those thousandths of a second add up, so every few years we have to slip in an extra second to account for them. Without leap seconds, we'd eventually end up with noon at 7 o'clock, though admittedly, this would take a very long time.\"],\n",
       " 'answers.score': [44, 5, 4],\n",
       " 'answers.text_urls': [[],\n",
       "  ['http://adminhacks.com/leap-second-bugs.html'],\n",
       "  ['https://en.wikipedia.org/wiki/Newcomb%27s_Tables_of_the_Sun']],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## extract all text subfield for example here from a_id and text up to 1 level, convenient for processing\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462e418343a840da9f1ccd76e29c5fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/91772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2086 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1508 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1823 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc47279480044e8b3c3c020814f7f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2655 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1493 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2147 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1401 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6958fbfc1f45a0a25202bf7ef71ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1861 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1244 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2557 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1170 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b4cbcb22a043e7b13d1378469b3f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5411 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1247 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4998 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(preprocess_function, batched=True, num_proc=4, remove_columns=eli5[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604970e2e3644fbca76eeeb2887a8336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/91772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fab36cf2eb645599c037a8cb3a992a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a848cda54c5543ac9e0f53b6869f5cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e46b747baec4efa9308316344282719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5411 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad61d1ba3414c79bd110087137e1013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e9dff44ca04fbdbbd13c66909a77d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a4473d07941b3b3f57784977996bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /home/phamt11/anaconda3/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\n",
      "Requirement already satisfied: transformers in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (0.27.2)\n",
      "Requirement already satisfied: safetensors in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.4.0)\n",
      "Requirement already satisfied: sympy in /home/phamt11/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/phamt11/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/phamt11/anaconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "from transformers import AutoModelForMultipleChoice, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"a_random_string\", inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained('microsoft/deberta-v3-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 435,849,217 || trainable%: 0.18043671281850668\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## With this function, we find the absolute max of the data X and scale it back to that max. From floating point 16 bit to int 8 bit (-127, 127)\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_metadata": {
   "author": "Leonardo Espinosa",
   "title": "MLPP: Supervised Learning"
  },
  "livereveal": {
   "overlay": "<div class='myheader'><h2 class='headertekst'>Machine Learning for Predictive Problems. Big Data Analytics Specialization <img src='./images/00_arcada.png' width='200' height='50'></img></h2><h3 ><a href='#/3/1'>(index)</a></h3></div>",
   "progress": true,
   "scroll": true,
   "theme": "serif",
   "transition": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
